import wikipedia
import wikipediaapi
import os
import time
import random
from tqdm import tqdm

# åˆå§‹åŒ– Wikipedia API
wiki_wiki = wikipediaapi.Wikipedia(language='en', user_agent='FengLuo(fl38@rice.edu)')

def clean_keyword(keyword):
    """
    æ¸…ç†å…³é”®è¯ï¼Œç§»é™¤æ— å…³çš„åç¼€ï¼Œå¹¶è½¬æ¢æˆæ›´é€‚åˆ Wikipedia çš„æ ¼å¼ã€‚
    """
    words = keyword.split()
    # ç§»é™¤å¸¸è§çš„åŸå¸‚ç±»å‹åç¼€
    remove_suffixes = {"CDP", "city", "town", "village", "municipality", "borough"}
    cleaned_words = [word for word in words if word not in remove_suffixes]
    cleaned_keyword = " ".join(cleaned_words)
    return f"{cleaned_keyword}, {words[-1]}"  # ç¡®ä¿æ ¼å¼ä¸º "City, State"

def fetch_wikipedia_data(keyword, output_dir, log_file):
    """
    çˆ¬å– Wikipedia é¡µé¢ï¼Œä½¿ç”¨æ¨¡ç³Šæœç´¢å¤„ç†æ‰¾ä¸åˆ°çš„é¡µé¢æƒ…å†µã€‚
    """
    time.sleep(random.uniform(1, 3))  # éšæœºå»¶è¿Ÿï¼Œé˜²æ­¢ API è¿‡è½½

    # æ¸…ç†å…³é”®è¯ï¼Œç¡®ä¿æ ¼å¼æ­£ç¡®
    cleaned_keyword = clean_keyword(keyword)

    # å…ˆå°è¯•ç²¾ç¡®æŸ¥æ‰¾
    page = wiki_wiki.page(cleaned_keyword)

    # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•æ¨¡ç³Šæœç´¢
    if not page.exists():
        search_results = wikipedia.search(cleaned_keyword)
        if not search_results:
            with open(log_file, "a", encoding="utf-8") as log_f:
                log_f.write(f"{keyword}\n")
            print(f"âŒ No Wikipedia page found for '{keyword}', logged in {log_file}.")
            return
        best_match = search_results[0]
        print(f"ğŸ” Using best match: {best_match}")
        page = wiki_wiki.page(best_match)

    if page.exists():
        file_path = os.path.join(output_dir, f"{page.title.replace(' ', '_')}.txt")
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(f"Title: {page.title}\n")
            f.write(f"URL: {page.fullurl}\n\n")
            f.write(f"Summary:\n{page.summary}\n\n")
            f.write(f"Full Content:\n{page.text}\n")
        print(f"âœ… Saved: {file_path} (Summary Length: {len(page.summary)})")
    else:
        with open(log_file, "a", encoding="utf-8") as log_f:
            log_f.write(f"{keyword}\n")
        print(f"âŒ No Wikipedia page found for '{keyword}', logged in {log_file}.")

def process_keywords(input_file, output_dir, log_file):
    """
    è¯»å–åŸå¸‚åç§°ï¼Œå»é‡ï¼Œå¹¶æŸ¥è¯¢ Wikipediaã€‚
    """
    os.makedirs(output_dir, exist_ok=True)
    
    with open(input_file, "r", encoding="utf-8") as f:
        keywords = [line.strip() for line in f.readlines()]

    print(f"ğŸ“Œ Processing {len(keywords)} keywords...")

    for keyword in tqdm(keywords):
        print(f"ğŸ” Fetching: {keyword}")
        fetch_wikipedia_data(keyword, output_dir, log_file)


if __name__ == "__main__":
    input_file = "data/unique_cities.txt"  # å…³é”®è¯æ–‡ä»¶
    output_dir = "data/wiki/city/"  # è¾“å‡ºç›®å½•
    log_file = "data/missing_pages.log"  # è®°å½•æœªæ‰¾åˆ°çš„é¡µé¢

    process_keywords(input_file, output_dir, log_file)
